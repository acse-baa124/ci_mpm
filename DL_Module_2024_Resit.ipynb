{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoD2-TjHu-R8"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YLNtm8gNsviTEnVXzfiby2VMKrc0XzLP\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejcKjnJDkx-"
      },
      "source": [
        "# Task 1\n",
        "## 1.1 Download and prepare MNIST\n",
        "\n",
        "Download MNIST and prepare it as you see fit to be used in the following sections. Explain what you do at each step and **WHY** you do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V2auK8fSDlIy",
        "outputId": "863f2037-67ec-465f-d132-b0d095b193c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available!\n"
          ]
        }
      ],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))]) # The original implementation of GANs scales the data\n",
        "                                                  # between [-1, 1]. We do that through the normalise transform.\n",
        "                                                  # Removing 0.5 from the data scales the data from [0, 1] to [-0.5, 0.5]\n",
        "                                                  # Dividing it by 0.5 scales the data from [-0.5, 0.5] to [-1, 1]\n",
        "\n",
        "train_dataset = MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='./mnist_data/', train=False, transform=transform, download=True)\n",
        "#taken from lectures on day 7"
      ],
      "metadata": {
        "id": "Bceu8z5xn-X5",
        "outputId": "68af5e51-ea18-49ed-8a33-b89cb397303a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 540kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.00MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.21MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3OdgYjEDxFt"
      },
      "source": [
        "## 1.2 Modify your prepared dataset as follows:\n",
        "\n",
        "- flip horizontally half the images in both the train and test datasets.\n",
        "- modify the labels to reflect if the images are original or flipped.\n",
        "\n",
        "There are several ways to do this, and more than one right way to do it. Explain your implementation choices for these MNIST modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfkKbVlnDzTi"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-OEl6kxEAHo"
      },
      "source": [
        "## 1.3 Split the dataset\n",
        "\n",
        "Split the data as you see fit, so that you can use it in the following sections.\n",
        "\n",
        "Plot a few samples with their new corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7zDMt6bEDHP"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrC8q88KEJ5c"
      },
      "source": [
        "## 1.4 Implement a CNN classifier\n",
        "\n",
        "The classifier should output whether a sample (MNIST image) is flipped or not.\n",
        "\n",
        "Provide evidence of the performance of your classifier (training curves, confusion matrices, and anything else you think is pertinent) and discuss what the evidence is telling you about your chosen model, the dataset, and the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZVbRzO2EMJf"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ETlybgnEQXq"
      },
      "source": [
        "## 1.5 Hyperparameter strategy\n",
        "\n",
        "Choose one hyperparameter to tune with three different values. Why did you choose this hyperparameter?\n",
        "\n",
        "If you had more time, what other hyperparameters would you explore to improve the performance of your network? Choose two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvgbHfuaETAH"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSPkIADQFc4j"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQZtHf7jEZHn"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "## 2.1 Generative modelling\n",
        "\n",
        "Use the MNIST dataset you created in task 1 to implement two generative models (you can reuse code from the lectures): a VAE, and a GAN.\n",
        "\n",
        "Once implemented, train each of them for 10 mins (a few epochs). Use the models to generate new samples (they do not need to be good, of course) and plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3iJ6rxwEd4b"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0vfclFtEhDK"
      },
      "source": [
        "Then, answer the following:\n",
        "\n",
        "## 2.2\n",
        "\n",
        "What is the main idea behind a VAE? How does it generate new samples from a training dataset? Explain how it is different than a regular autoencoder, and what is the role that the KL loss term plays in it. Refer to the code you implement in 2.1 in your explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzGb5qi0Em-r"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6MxxaV3ElkC"
      },
      "source": [
        "## 2.3\n",
        "In GANs there are two competing networks, what role do they play in the training process? And how do they learn to generate samples differently from a VAE? Refer to the code you implement in 2.1 in your explanations.\n",
        "\n",
        "Explain why the loss evolution for generator and discriminator oscillate during training. Is there a way to clearly determine when training is complete?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_dVd-ngEqBE"
      },
      "outputs": [],
      "source": [
        "# Add as many text and code blocks as needed to answer the question here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDfgid9_Fbjn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpxtjvN-Es9k"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "## 3.1 RNNs\n",
        "Given an RNN, with a single layer, that has been trained and is used to generate new outputs, like the one in the figure below:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1GmEyTghDKdkBx0gbFrw_KOn6zuKbNaHc\" width=\"500\"/></center>\n",
        "\n",
        "[link to the figure](https://drive.google.com/file/d/1GmEyTghDKdkBx0gbFrw_KOn6zuKbNaHc/view?usp=share_link) in case you cannot see it in the notebook.\n",
        "\n",
        "<br>\n",
        "\n",
        "If the dimension of my hidden vector is 151, and the dimension of my input and output is 101, what are the sizes of the matrices $W_{hh}$, $W_{xh}$ and $W_{hy}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq0C8m7tE2nI"
      },
      "source": [
        "$W_{hh}$:\n",
        "- ... rows\n",
        "- ... columns\n",
        "\n",
        "$W_{xh}$:\n",
        "- ... rows\n",
        "- ... columns\n",
        "\n",
        "$W_{hy}$:\n",
        "- ... rows\n",
        "- ... columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBiKUUAE81H"
      },
      "source": [
        "*Add as many text blocks as needed to answer the question here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZgBF-ifE8sa"
      },
      "source": [
        "## 3.2 LSTMs\n",
        "Indicate which of the following statements is False, and explain why:\n",
        "\n",
        "a) LSTMs have two vectors that are passed: the cell state and the hidden state. The cell state is responsible for keeping 'longer term' memory in the system.\n",
        "\n",
        "b) The activations functions of the various gates in an LSTM are tanh, so that they can act as 'continuous switches' to pass or not pass information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgfVMSh0FBlm"
      },
      "source": [
        "*Add as many text blocks as needed to answer the question here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ChKGN2FCAn"
      },
      "source": [
        "## 3.3 Transformers\n",
        "\n",
        "- In the attention layers of a transformer, how are the keys, queries, and values calculated and combined? Explain what elements of the layer are learned during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5JgcJiFFZa"
      },
      "source": [
        "*Add as many text blocks as needed to answer the question here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM1bybksFFyQ"
      },
      "source": [
        "- After data has gone through the final linear layer of the translation transformer introduced in the paper 'Attention is all you need', what is the shape (dimensions) of the data and how is it converted into words (or tokens)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5O_NTD8FJhf"
      },
      "source": [
        "*Add as many text blocks as needed to answer the question here*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}